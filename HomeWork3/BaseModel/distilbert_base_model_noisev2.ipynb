{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1257b6c-6407-48a2-acc1-19eadbf69822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vnara/.conda/envs/pytorch_env1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from evaluate import load\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69db8eb-1f8f-40f5-9413-1e98ca6176dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_data_path = '../Dataset/spoken_train-v1.1.json'\n",
    "test_data_path = '../Dataset/spoken_test-v1.1_WER54.json'\n",
    "MAX_LENGTH = 512\n",
    "MODEL_PATH = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "epochs = 6 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c8fc30-fd2c-4bc7-aa31-31307c73761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        raw_data = json.load(f)\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for group in raw_data['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context'].lower()\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question'].lower()\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append({'text': answer['text'].lower(), 'answer_start': answer['answer_start'], 'answer_end': answer['answer_start'] + len(answer['text'])})\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = load_and_preprocess_data(train_data_path)\n",
    "valid_contexts, valid_questions, valid_answers = load_and_preprocess_data(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a8f55f-6805-4ef8-942f-260ce93ea480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_answers(questions, contexts, answers):\n",
    "    tokenized_inputs = tokenizer(questions, contexts, max_length=MAX_LENGTH, truncation=True, padding=True, return_offsets_mapping=False)\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for idx, answer in enumerate(answers):\n",
    "        answer_encoding = tokenizer(answer['text'], max_length=MAX_LENGTH, truncation=True, padding=True)\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "        for pos in range(len(tokenized_inputs['input_ids'][idx]) - len(answer_encoding['input_ids']) + 1):\n",
    "            match = True\n",
    "            for i in range(1, len(answer_encoding['input_ids']) - 1):\n",
    "                if answer_encoding['input_ids'][i] != tokenized_inputs['input_ids'][idx][pos + i]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                start_position = pos + 1\n",
    "                end_position = pos + i + 1\n",
    "                break\n",
    "        start_positions.append(start_position)\n",
    "        end_positions.append(end_position)\n",
    "\n",
    "    tokenized_inputs.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_encodings = tokenize_and_align_answers(train_questions, train_contexts, train_answers)\n",
    "valid_encodings = tokenize_and_align_answers(valid_questions, valid_contexts, valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61424143-8df1-4244-b069-05aaa66d9b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/vnara/.conda/envs/pytorch_env1/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class QuestionAnswerDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = QuestionAnswerDataset(train_encodings)\n",
    "valid_dataset = QuestionAnswerDataset(valid_encodings)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ada43d3-012b-4d1f-b3ab-8c790a76646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_fn(start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    start_probabilities = softmax(start_logits)\n",
    "    inv_start_probabilities = 1 - start_probabilities\n",
    "    end_probabilities = softmax(end_logits)\n",
    "    inv_end_probabilities = 1 - end_probabilities\n",
    "    \n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "    log_start_probabilities = log_softmax(start_logits)\n",
    "    log_end_probabilities = log_softmax(end_logits)\n",
    "    \n",
    "    negative_log_likelihood_loss = nn.NLLLoss()\n",
    "    \n",
    "    focal_loss_start = negative_log_likelihood_loss(torch.pow(inv_start_probabilities, gamma) * log_start_probabilities, start_positions)\n",
    "    focal_loss_end = negative_log_likelihood_loss(torch.pow(inv_end_probabilities, gamma) * log_end_probabilities, end_positions)\n",
    "    \n",
    "    return (focal_loss_start + focal_loss_end) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e630e95e-1f58-4030-8b40-f75c6498fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "        \n",
    "        loss = focal_loss_fn(start_logits, end_logits, start_positions, end_positions, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        start_pred = torch.argmax(start_logits, dim=1)\n",
    "        end_pred = torch.argmax(end_logits, dim=1)\n",
    "        start_accuracy = (start_pred == start_positions).sum() / len(start_pred)\n",
    "        end_accuracy = (end_pred == end_positions).sum() / len(end_pred)\n",
    "        accuracies.append(start_accuracy.item())\n",
    "        accuracies.append(end_accuracy.item())\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc29d6f7-a806-4e78-90fe-14d771e03d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f65ca0-23bd-48f1-a298-8cd296d1191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_calc(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f49e33-12fc-48d2-81f5-c87e69177e37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from distilbert_qa_base_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 17841/17841 [01:28<00:00, 201.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.2828, WER Score: 3.4903\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    f1_scores = []\n",
    "    answer_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_true = batch['start_positions'].to(device)\n",
    "            end_true = batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "\n",
    "            start_pred = torch.argmax(start_logits, dim=1)\n",
    "            end_pred = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "            for i in range(input_ids.size(0)):\n",
    "                pred_answer = tokenizer.decode(input_ids[i][start_pred[i]:end_pred[i]+1])\n",
    "                true_answer = tokenizer.decode(input_ids[i][start_true[i]:end_true[i]+1])\n",
    "                answer_list.append([pred_answer, true_answer])\n",
    "                f1_score_value = f1_score_calc(pred_answer, true_answer)\n",
    "                f1_scores.append(f1_score_value)\n",
    "\n",
    "    for ans_pair in answer_list:\n",
    "        if len(ans_pair[0]) == 0:\n",
    "            ans_pair[0] = \"$\"\n",
    "        if len(ans_pair[1]) == 0:\n",
    "            ans_pair[1] = \"$\"\n",
    "        predictions.append(ans_pair[0])\n",
    "        references.append(ans_pair[1])\n",
    "\n",
    "    wer_metric = load(\"wer\")\n",
    "    wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "    return avg_f1_score, wer_score\n",
    "\n",
    "f1_scores = []\n",
    "wer_scores = []\n",
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "model_save_path = 'distilbert_qa_base_model'\n",
    "\n",
    "if os.path.exists(model_save_path):\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_save_path).to(device)\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_save_path)\n",
    "    print(f'Model and tokenizer loaded from {model_save_path}')\n",
    "    f1_score, wer_score = evaluate(model, valid_loader)\n",
    "    print(f'F1 Score: {f1_score:.4f}, WER Score: {wer_score:.4f}')\n",
    "else:\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = train(model, train_loader, optimizer)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        print(f\"Epoch {epoch + 1}, Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        f1_score, wer_score = evaluate(model, valid_loader)\n",
    "        f1_scores.append(f1_score)\n",
    "        wer_scores.append(wer_score)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, F1 Score: {f1_score:.4f}, WER Score: {wer_score:.4f}\")\n",
    "\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
