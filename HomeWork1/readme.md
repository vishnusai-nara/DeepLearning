
HW1 1  Deep vs Shallow

HW1-1.1 Simulate a Function: 

https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_1_1_Simulate_Function.ipynb


Function_1: 

For first function, Sin(2x) + Cos (x) is used which is a non - linear single input and output function. The training data is created by randomly picking numbers between -3 and 3, with each number having an equal chance of being chosen. The validation data  is generated by evenly spreading numbers across a range.

I have used three MLP DNN models to simulate the function. The models differ from hidden layers count & neurons to explore the effects on learnings. The Mean Squared Error (MSELoss) is used as the loss function, SGD as the optimizer, learning rate as le-2 and Leaky ReLU as the activation function across all three models. We have trained the models by using the data.

Results:
Model 1, with 4 hidden layers and 5831 parameters, converged at epoch 3521
with a loss of 0.01548. Model 2, featuring 3 hidden layers and 4011 parameters, reached convergence at epoch 3246 with a loss of 0.02914. Model 3, with only 1 hidden layer and 1141 parameters, converged at epoch 2156 with a loss of 0.05103. Convergence of models happens after the epoch limit reaches that is when the model learns at a slow rate.

Model 1, with the lowest loss (0.01548), generalizes best despite slower convergence. Model 3 is faster but less accurate (loss 0.05103), while Model 2 is average. Overall, Model 1 is the best performer.


Function_2: 
For the second function, sinc(5 * x) is used which is a non - linear single input and output function. The training data is created by randomly picking numbers between -1.5 and 1.5, with each number having an equal chance of being chosen. The validation data  is generated by evenly spreading numbers across a range.

I have used three MLP DNN models to simulate the function. The models differ in hidden layers and neurons to examine their impact on learning. The Mean Squared Error (MSELoss) is used as the loss function, SGD as the optimizer, learning rate as le-2 and Leaky ReLU as the activation function across all three models. We have trained the models by using the data.

Results:
Model 1, with 6 hidden layers and 571 parameters, converged at epoch 39 with a loss of 0.06130. Model 2, featuring 3 hidden layers and 556 parameters, reached convergence at epoch 58 with a loss of 0.00595. Model 3, with only 1 hidden layer and 547 parameters, converged at epoch 242 with a loss of 0.01383. Convergence of models happens after the epoch limit reaches that is when the model learns at a slow rate.

Model 2, with the lowest loss (0.00595) and 58 epochs to converge, generalizes best. Model 1 converged fastest (39 epochs) but had the highest loss (0.06130), while Model 3 took the longest (242 epochs) with a higher loss (0.01383). Overall, Model 2 is the best, balancing accuracy and efficiency.

HW1-1.2 Train on Actual Tasks: 

https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_1_2_Train_on_Actual_Task.ipynb

Created  3 CNN models, differing in their architectures were designed for CIFAR-10 classification containing training and test batch sizes as 100 and 64. The hyperparameters used are a learning_rate of 0.001, CrossEntropyLoss as the loss_function, Adam_optimizer, 15 epochs and for activation Leaky_Relu for intermediate layers and log softmax for output, kernel size 3*3.
CNN Models: 
Model
CNN Model 1
CNN Model 2
CNN Model 3
Convolutional Layers
3 layers with 16, 32, 64 filters
3 layers with 32, 64, 128 filters
4 layers with 64, 128, 256, 256 filters
Batch Normalization
After each conv layer (16, 32, 64 filters)
After each conv layer (32, 64, 128 filters)
After each conv layer (64, 128, 256, 256 filters)
Max-Pooling
(2x2 window)
After each conv layer 
After each conv layer
After 1st, 2nd, and 4th conv layers 
Dropout
None 
None
0.25 before 1st fully connected layer, 0.5 before final layer
2 Fully Connected Layers
120, 10 neurons
 256, 10 neurons
512, 10 neurons


Observation:
Based on the graphs, Model 2 appears to be the most suitable for training data. It shows a clear reduction in loss over time, indicating that it is learning and improving during the training process. However, its accuracy fluctuates significantly across epochs, suggesting instability in predictions. In contrast, Model 1 and Model 3 exhibit minimal loss improvement, staying constant, which indicates that they are not learning effectively from the training data.
Although Model 2 is better suited for training, the fluctuating accuracy across all models points to a potential need for further tuning, such as optimizing hyperparameters or adjusting the model architecture, to stabilize learning and improve overall accuracy.


HW1 2 Optimization:
HW1 2-1 Visualize the optimization process:
https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_2_1.ipynb
The model has 784 input units (for 28x28 MNIST images), two hidden layers with 128 and 64 neurons, and 10 output neurons for digit classification. Leaky_ReLU is used as the activation function. The MNIST dataset is normalized (mean 0.5, std 0.5), and DataLoader uses a batch size of 64. Training runs for 8 sessions of 24 epochs, recording weights every 3 epochs. SGD optimizer (learning rate 0.001) and CrossEntropyLoss are used, with first-layer and all-layer weights stored during training.
After training, PCA is applied to the full model's weights and first-layer weights to reduce dimensionality and capture key patterns. The two principal components provide a simplified view of weight relationships and dominant patterns in the data.
The PCA-reduced weight graphs make it easier to interpret the model's structure and weight evolution in both the full model and first layer, offering insights into weight distribution, clustering, and potential performance improvements.


Observation:
The PCA-reduced graphs show distinct clustering, with the full model capturing more complex patterns than the first layer. Both layers exhibit structured weight distributions, indicating specialization in different aspects of data processing.

HW1 2.2 Observe gradient norm during training:
https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_2_2.ipynb
The model uses the hyperbolic cosine function, exp(x) as a non-linear transformation for a single input and output. The training dataset is uniformly sampled from -5 to 5 with a 0.001 step size, while the validation dataset consists of evenly spaced values.

Observation:
Gradient norm spikes initially, indicating large updates early in training, then gradually stabilizes as the model converges. The loss decreases rapidly in the first 1000 epochs, then flattens, showing the model is effectively minimizing error and has reached convergence with minimal further improvements. From these graphs we can say that the training process is working as expected, with the model converging to an optimal solution over time as the gradients and loss stabilizes.



HW1 2.3 What happens when the gradient is almost zero? 
https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_2_3.ipynb
The model uses the hyperbolic cosine function, exp(x) as a non-linear transformation for a single input and output. The training dataset is uniformly sampled from -3 to 3 with a 0.001 step size, while the validation dataset consists of evenly spaced values.
The DNN model consists of an input layer, five hidden layers with linear transformations and Leaky ReLU activations, and a single-neuron output layer. The model is trained for 100 iterations, monitoring gradient norms and calculating the minimal ratio from the Hessian's eigenvalues. Training includes parameter initialization, backpropagation, gradient updates, and convergence check (threshold: 0.001). Loss values and minimal ratios are recorded for analysis.

Observations:
Graphs and experiments show that after 100 training runs of 100 epochs, the loss consistently approached zero, indicating successful learning. The minimal ratio around 0.001 suggests a stable optimization, with a very small ratio of positive eigenvalues in the Hessian matrix, confirming that the model reached an optimal solution with stable convergence.

HW1- 3 Generalization:
HW1 3.1 Can network fit random labels?
https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_3_1.ipynb
The CNN model has three convolutional layers (16, 32, 64 filters, 3x3 kernels, padding=1) with 2x2 max pooling, followed by two fully connected layers (120 and 10 neurons). ReLU is used for activation, with log softmax on the output. The dataset has 60,000 32x32 images across 10 classes, normalized by mean and standard deviation. Randomized training labels complicate classification as they no longer match the images.
Model trained with 100 epochs - batch size of 100, 64 for training and testing, using CrossEntropyLoss and Adam optimizer with LR 0.001. Training loss is recorded each epoch, and test loss is evaluated after each epoch.

Observation:
The graph shows a steady decrease in training loss, indicating effective learning. However, the test loss increases, suggesting overfitting as the model fails to generalize to unseen data. Techniques like regularization, dropout, or early stopping could help improve generalization and prevent overfitting.
HW1 3.2 Number of parameters vs Generalization:
https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_3_2.ipynb
The CNN models vary in the no of channels in the convolutional layers & neurons in the fully connected layers. Net_1 starts with 16 channels and 120 neurons, while Net_10 has 160 channels and 1200 neurons, with the other models incrementally increasing in both channels and neurons. All models use the CIFAR-10 dataset, where images are transformed into tensors and normalized.
The models are trained for 10 epochs using the Adam optimizer with a learning rate of 0.001, and CrossEntropyLoss as the loss function. The batch size is set to 100 for training and 64 for testing. Each model's performance is evaluated under the same training and testing conditions to compare how the varying architectures impact learning and generalization.

Observation:
The graphs show that as the number of parameters increases, training loss decreases and training accuracy improves, indicating better performance on the training data. However, test loss remains higher and test accuracy improves less consistently, suggesting overfitting. Larger models with more parameters tend to perform well on training data but struggle to generalize with test data, highlighting the need for regularization to enhance generalization.

HW1 3.3 Flatness vs Generalization part-1:
https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_3_3_1.ipynb
The model is trained on the CIFAR-10 dataset, with images converted to tensors and normalized. The CNN includes three convolutional layers with 16, 32, and 64 filters (3x3 kernels, padding=1), followed by ReLU activation and 2x2 max-pooling. Two fully connected layers follow, with 120 neurons in the first and 10 in the second.
Training is done twice with batch sizes of 500 and 300, using CrossEntropyLoss and the Adam optimizer (learning rate 0.001) for 15 epochs per model. Afterward, parameters are linearly interpolated over 100 steps with alpha values from 0 to 1. For each interpolated model, training loss and test accuracy are evaluated to compare the performance of both batch sizes.

Observation:
As per the above plot, the training loss diminishes and training accuracy rises indicating effective learning. However, the test loss initially decreases but then rises, with test accuracy plateauing, suggesting overfitting and poor generalization to unseen data. With accuracy peaking below 35%, the models may need architectural adjustments, more diverse training data, or hyper parameter tuning to improve their performance.


HW1 3.3 Flatness vs Generalization part-2:
https://github.com/vishnusai-nara/DeepLearning/blob/main/HomeWork1/NVS_HW1_3_3_2.ipynb
The CIFAR-10 dataset, consisting of color images across ten classes, is divided into training and testing sets. The CNN includes a convolutional layer, two fully connected layers, ReLU activation, batch normalization, and log-softmax for output predictions.
The experiments explore different optimizers with both Adam and SGD and  with various learning rates, using cross-entropy loss. Batch sizes of 64, 128, and 256 are tested over 10 epochs. During training, loss, accuracy, and sensitivity we have measured output changes with respect to input changes and are tracked. Multiple configurations of batch_size, optimizers, and learning_rates are used to assess the performance across 10 epochs.

Observation:
Varying batch sizes (64, 128, 256) affect convergence speed and stability, with larger batches converging faster but potentially harming generalization, as seen by stagnating test accuracies. Adam generally provides smoother, more stable optimization compared to SGD, leading to more consistent reductions in both training and testing losses. Higher learning rates lead to quicker initial loss reduction but may cause instability and erratic performance in both training and testing metrics.
