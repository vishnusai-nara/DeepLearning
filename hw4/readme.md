
Generative Adversarial Networks:
This task involves the design and training of 2 models, the first one like a generator and the second  one is a discriminator—using advanced techniques like Wasserstein GAN, Deep Convolutional GAN(DCGAN), and  Auxiliary Classifier(ACGAN). The models aim to improve training stability and make sure that the produced images look realistic & correspond correctly to their designated categories.
Dataset:
The CIFAR-10 dataset, used here, contains 60,000 32x32 pixel color images divided into 10 classes, such as cars, birds, dogs, and others.
GitHub Link: https://github.com/vishnusai-nara/DeepLearning/tree/main/hw4
1) DCGAN:
DCGAN excels at generating high-quality synthetic images and extracting features from visual data. It uses convolutional layers in the discriminator and transposed convolutional layers in the generator, ensuring stable and efficient training on image datasets.
Generator: Synthetic images are generated by processing textual input into a dense vector combined with random noise for diversity. This vector is reshaped into a multidimensional space as an initial feature map, refined through transposed convolutional layers that expand dimensions and reduce depth. Stability is ensured with batch normalization and ReLU, and the final features are normalized using the Tanh activation function to align with the input text.
Discriminator:The discriminator evaluates if an image with textual data is real or generated. It converts the text into a feature vector, processes the image through convolutional layers with Leaky ReLU and batch normalization, and merges the extracted features with text features. The final set passes through a convolutional layer to predict authenticity and ensure alignment with the text.
Hyperparameters: Using the Adam optimizer, models train with 50 epochs, a 0.0002 learning rate, a 128 batch size, a 100-dimensional noise vector, and 256-dimensional text encoding, with BCELoss evaluating performance.
2) WGAN:
The Wasserstein GAN (WGAN) improves the GAN training process by providing more meaningful gradients and creating a stronger link between the loss function and the generated image quality compared to traditional GANs. Instead of using a discriminator, WGAN employs a critic that evaluates the "realness" of images on a continuous scale rather than a binary decision. The critic is optimized to converge during each generator update step, ensuring efficient and stable training.
Generator:Synthetic image creation is facilitated by combining a noise vector with a dense vector derived from textual input, processed through a linear layer with ReLU activation. The resulting vector is mapped to a higher-dimensional space and reshaped into an image-like structure. It is then progressively enhanced through multiple transposed convolutional layers with batch normalization and ReLU activation, which enlarge spatial dimensions while reducing depth. Finally, a transposed convolutional layer adjusts the tensor to match the desired dimensions, with color channels normalized using a Tanh activation function.
Discriminator (Critic): The critic takes textual input and transforms it into a vector representation through a linear layer with ReLU activation. Simultaneously, the input image undergoes downsampling via convolutional layers, aided by batch normalization and Leaky ReLU for feature extraction. These features are then combined with the text embedding to create a unified representation. This combined representation is then passed through a concluding linear layer, yielding a scalar output that indicates the critic's judgment of the image's genuineness.
Gradient Penalty: WGAN employs a gradient penalty to ensure stable training by penalizing the critic if the gradients for real and fake images deviate excessively. This stabilizes the critic’s ability to evaluate images, minimizing over-optimization and improving the model's reliability during training.
Hyperparameters: The model trains for 50 epochs using the Adam optimizer (learning rate: 0.0002, batch size: 128). A 100-dimensional noise vector and a gradient penalty of 10 are used, with BCELoss as the loss function. The critic runs five iterations per generator update to ensure balance.
3) ACGAN:
ACGAN enhances traditional GAN structures by integrating a classification layer into the discriminator. This enhancement enables the discriminator to assess both image authenticity and class labels, allowing the generator to create images that are realistic and accurately classified, thereby improving specificity and diversity in the outputs. By aligning generated images to specific classes, ACGAN merges generative and classification tasks into a unified framework.
Generator: The generator begins with an embedding layer that transforms class labels into dense vectors. These vectors are then merged with a noise vector to guide the image creation process. The resolution of the generated image is incrementally enhanced through transposed convolutional layers, reinforced with batch normalization and ReLU activations to ensure consistent and effective training. The process concludes with a transposed convolutional layer that adjusts the image's dimensions and color scale, normalizing pixel values through a Tanh activation function.
Discriminator: The discriminator downscales input images through multiple convolutional layers to extract features. These layers employ batch normalization and Leaky ReLU activations for effective feature extraction. The discriminator outputs two results: one for real or generated classification and another for predicting the image's class label. The real-or-fake output uses a sigmoid function, while the class prediction utilizes a log-softmax function to assign the image to a specific category. This dual-output configuration ensures the model generates realistic images aligned with predefined class labels.
Hyperparameters: ACGAN training runs for 50 epochs with the Adam optimizer, a 0.0002 learning rate, and a batch size of 128. The noise vector is set to a dimension of 100, and the model is designed to classify 10 categories. 64 feature maps are used for both models. For real/fake classification, Binary Cross-Entropy Loss (BCELoss) is applied, while Negative Log-Likelihood Loss (NLLLoss) is used for class predictions. These settings ensure effective learning and high-quality class-conditional image generation.
Frechet Inception Distance (FID) Calculation:
FID is a key metric for evaluating model-generated images against real ones. It evaluates the resemblance by analyzing the statistical distributions of features extracted from real and generated images using the InceptionV3 network. FID specifically calculates the distance between the feature distributions of the two datasets within a complex, high-dimensional space. Generated images that closely resemble real ones are reflected by a lower FID score, demonstrating improved model performance in creating realistic and high-quality outputs. This makes FID an effective and reliable tool for evaluating the effectiveness of generative models.
Metrics:
Loss Graph:
During training, generator and discriminator losses are tracked to assess performance. The loss of the generator indicates its effectiveness in deceiving the discriminator by creating realistic images. Conversely, the discriminator's loss indicates its ability to distinguish real images from generated ones. Tracking these losses over time provides valuable insights into the stability and progress of the training process, revealing whether the models are learning effectively or encountering issues like mode collapse or overfitting.
Frechet Inception Distance (FID) Graph:
The Frechet Inception Distance(FID) metric calculates the correspondence within the feature patterns of authentic & synthesized images, featuring smaller values FID scores signifying improved quality and realism in the generated outputs. Visualizing the FID scores during training highlights the model's performance improvements, showing how closely the generated images resemble real ones as the training advances. This metric is particularly helpful for evaluating the generator's ability to produce visually convincing and statistically accurate outputs.

FID Comparison Analysis:

The graph compares the FID-scores of 3 Generative Adversarial Network (GAN) models—WGAN, DCGAN & ACGAN on  50 epochs, where lower FID scores represent higher image quality.
DCGAN: DCGAN consistently achieves the lowest FID scores across most epochs, highlighting its strong ability to generate high-quality images throughout the training process. This suggests that DCGAN maintains a stable and efficient performance when producing realistic images.
WGAN: WGAN displays significant fluctuations in its FID scores, with generally higher values. This indicates reduced stability in its performance and challenges in reliably generating high-quality images.
ACGAN: While ACGAN initially surpasses WGAN with lower FID scores, it exhibits increased variability in the later epochs. This results in higher FID scores toward the end, potentially signaling issues such as overfitting or instability in the later stages of training.
In conclusion, DCGAN stands out as the most effective model for generating realistic images, as evidenced by its consistently superior FID performance throughout the training process.

